Matrix-matrix multiplication (\Gemm) is frequently used as a simple example with which to raise awareness of how to optimize code on modern processors.  The reason is that the operation is simple to describe, challenging to fully optimize, and of practical importance.  In this paper, we walk the reader through the techniques that underly the currently fastest implementations for CPU architectures.

\subsection{Basic Linear Algebra Subprograms (BLAS)}

The Basic Linear Algebra Subprograms (BLAS)~\cite{BLAS1,BLAS2,BLAS3,BLIS-Encycl} are an interface to a set of linear algebra operations upon which higher level linear algebra libraries.  Of these, the level-3 BLAS constitute a set of matrix-matrix operations that, if all matrix operands are $ n \times n$ in size, $ O( n^3 ) $ computation is performed over $ O( n^2 ) $ data so that the cost of moving data between memory layers (main memory, the caches, and the registers) can be amortized over many computations.  As a result, high performance can often be achieved if these operations are carefully implemented.

\subsection{Matrix-matrix multiplication (\Gemm)}

In particular, general matrix-matrix multiplication (\Gemm) with double precision floating point numbers is supported by the BLAS with the call
\begin{center}
	\tt dgemm( transa, transb, m, n, k alpha, A, lda, B, ldb, beta, C, ldc )
\end{center}
which, by appropriately choosing {\tt transa} and {\tt transb}, 
computes 
\[
C := \alpha A B + \beta C; \quad
C := \alpha A^T B + \beta C; \quad
C := \alpha A B^T + \beta C; \quad \mbox{or }
C := \alpha A^T B^T + \beta C.
\]
Here $ C $ is $ m \times n $ and $ k $ is the ``third dimension''.
In this paper we focus on optimizing a simplified version of this operation, namely $ C := A B + C $ where $ C $ is $ m \times n $, 
$ A $ is $ m \times k $, and $ B $ is $ k \times n $.

\subsection{High-performance implementation}

The intricacies of high-performance implementations are such that implementation of the BLAS in general and \Gemm\ in particular was 
often relegated to 
unsung experts who develop numerical libraries for the hardware vendors, for example as part of IBM's ESSL, Intel's MKL, Cray's LibSci, and AMD's ACML libraries.  In the past,
these libraries were often written at least partially in assembly.

The key paper~\cite{IBM:P2} showed how an ``algorithms and architectures" approach to hand-in-hand designing architectures, compilers, and algorithms allowed 
BLAS to be written in a high level language (Fortan) for the IBM Power architectures.
The Portable High Performance ANSI C (PHiPAC)~\cite{PHiPAC97} project subsequently provided guidelines for writing high-performance code in C and suggested how to autogenerate and tune \Gemm\ written this way.  The Automatically Tuned Linear Algebra Software (ATLAS)~\cite{ATLAS,ATLAS_journal} built upon these insights and made autotuning and autogeneration of BLAS libraries mainstream.

As part of the current paper we discuss more recent papers on the Goto approach to implementing \Gemm~\cite{Goto:2008:AHP} and the BLIS refactoring of that approach~\cite{BLIS1}, as well as other papers that are of more direct relevance.  

\subsection{Other similar exercises}

There are others who have put together exercises based on {\Gemm}.
Recent efforts relevant to this paper are
\myhref{http://apfel.mathematik.uni-ulm.de/~lehn/sghpc/gemm/index.html}{GEMM: From Pure C to SSE Optimized Micro Kernels}
by Michael Lehn at Ulm University and  a wiki on 
\myhref{http://wiki.cs.utexas.edu/rvdg/OptimizingGemm}{Optimizing Gemm} that we ourselves put together.
 

\subsection{We need you!}

The purpose of this paper is to guide you towards high-performance
implementation of \Gemm.  Our ulterior motive is that our BLIS framework for implementing BLAS requires a so-called micro-kernel to be highly optimized for various CPUs.  In teaching you the basic techniques, we are hoping to identify  ``The One'' who will contribute the best micro-kernel.
Think of it of our version of "HPC's Got Talent".
Although  we focus in our description on optimization for the Intel Haswell architecture, the setup can be easily modified to instead help you (and us) optimize for other CPUs.  Indeed, BLIS itself supports architectures that include x86 processors by AMD and Intel, IBM's Power processors, ARM processors, and DSP processors~\cite{BLIS2,BLIS3,BLIS-TI}.
